Filename: /home/francesco/Desktop/SEMESTER_PROJECT_2/gnn_time/gca_time/notebook/./../gca_time/training.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    18    615.6 MiB    615.6 MiB           1   @profile
    19                                         def train(model_decoder, model_dyn, optimizer, device, scheduler, \
    20                                                   train_loader, test_loader, HyperParams, params_train, params_test, times):
    21                                             """Trains latent-decoder model
    22                                             """
    23    615.6 MiB      0.0 MiB           1       train_history = dict(train=[])
    24    615.6 MiB      0.0 MiB           1       test_history = dict(test=[])
    25    615.6 MiB      0.0 MiB           1       min_test_loss = np.Inf
    26                                             
    27    615.6 MiB      0.0 MiB           1       model_decoder.train()
    28    615.6 MiB      0.0 MiB           1       model_dyn.train()
    29    615.7 MiB      0.1 MiB           1       loop = tqdm(range(HyperParams.max_epochs))
    30                                          
    31                                         
    32   2112.8 MiB      0.0 MiB           2       for epoch in loop:
    33    615.7 MiB      0.0 MiB           1           train_rmse = total_examples = 0
    34    615.7 MiB      0.0 MiB           1           loss_train_mse = 0
    35                                                     
    36   1986.8 MiB      0.0 MiB           3           for (alpha_indx, alpha) in enumerate(params_train):
    37   1303.8 MiB      0.0 MiB           2               stn_vec = []
    38   1303.8 MiB      0.0 MiB           2               t_vec = []
    39   1303.8 MiB      0.1 MiB           2               stn = torch.zeros(HyperParams.bottleneck_dim, device=device)
    40   1303.8 MiB      0.0 MiB           2               stn_vec.append(stn)
    41   1303.8 MiB      0.0 MiB           2               t_vec.append(times[0])
    42                                                 
    43   1986.8 MiB      0.0 MiB           8               for (t_indx, t) in enumerate(times[1:]):
    44   1759.1 MiB      0.1 MiB           6                   num_integrations = round(float((times[t_indx+1])-times[t_indx])/HyperParams.dt)
    45   1759.1 MiB      0.0 MiB           6                   u_t = []
    46   1759.1 MiB      0.0 MiB          18                   for j in range(num_integrations):
    47   1759.1 MiB      0.4 MiB          24                       u_t = gaussian_process.eval_u_t(t_vec[-1], \
    48   1759.1 MiB      0.0 MiB          12                                   params_train[alpha_indx], HyperParams.T_f)
    49                                                             
    50   1759.1 MiB      0.0 MiB          12                       dyn_input = torch.cat((u_t.unsqueeze(0), stn), dim=0)
    51   1759.1 MiB      1.1 MiB          12                       stn_derivative = model_dyn(dyn_input)
    52                                                             # if t < times[2]:
    53                                                             #     stn_derivative.detach()
    54                                                             #     stn.detach()
    55                                                             #     u_t.detach()
    56   1759.1 MiB      0.0 MiB          12                       stn = stn + HyperParams.dt * stn_derivative
    57   1759.1 MiB      0.0 MiB          12                       t_vec.append(t_vec[-1] + HyperParams.dt)
    58                                                         # Go through the decoder
    59   1762.7 MiB      6.6 MiB           6                   data = get_data(train_loader, (alpha_indx*(len(times)-1)+t_indx))
    60                                                         #data = data.to(device)
    61   1762.7 MiB      0.0 MiB           6                   decoder_input = torch.cat((u_t.unsqueeze(0), stn), dim=0)
    62                                                         
    63   1771.0 MiB     49.0 MiB           6                   out = model_decoder(decoder_input, data)
    64   1986.8 MiB   1313.8 MiB           6                   loss_train_mse += physics_loss(data.x, out, HyperParams)
    65   1986.8 MiB      0.0 MiB           6                   total_examples += 1
    66                                                 
    67   1986.8 MiB      0.0 MiB           1           loss_train_mse = loss_train_mse/total_examples
    68   1986.8 MiB      0.0 MiB           1           optimizer.zero_grad()
    69   2088.7 MiB    101.8 MiB           1           loss_train_mse.backward()      
    70   2112.5 MiB     23.9 MiB           1           optimizer.step()
    71   2112.5 MiB      0.0 MiB           1           scheduler.step()
    72                                                
    73   2112.5 MiB      0.0 MiB           1           train_rmse = loss_train_mse
    74   2112.5 MiB      0.0 MiB           1           train_history['train'].append(train_rmse.item())
    75                                         
    76                                         
    77   2112.5 MiB      0.0 MiB           1           if HyperParams.cross_validation:
    78   2112.5 MiB      0.0 MiB           2               with torch.no_grad():
    79   2112.5 MiB      0.0 MiB           1                   test_rmse = total_examples = sum_loss = 0
    80   2112.5 MiB      0.0 MiB           1                   loss_test_mse = 0
    81   2112.5 MiB      0.0 MiB           2                   for (alpha_indx, alpha) in enumerate(params_test):
    82   2112.5 MiB      0.0 MiB           1                       stn_vec = []
    83   2112.5 MiB      0.0 MiB           1                       t_vec = []
    84   2112.5 MiB      0.0 MiB           1                       stn = torch.zeros(HyperParams.bottleneck_dim, device=device)
    85   2112.5 MiB      0.0 MiB           1                       stn_vec.append(stn)
    86   2112.5 MiB      0.0 MiB           1                       t_vec.append(times[0])
    87                                         
    88   2112.5 MiB      0.0 MiB           4                       for (t_indx, t) in enumerate(times[1:]):
    89   2112.5 MiB      0.0 MiB           3                           num_integrations = round(float((times[t_indx+1])-times[t_indx])/HyperParams.dt)
    90   2112.5 MiB      0.0 MiB           3                           u_t = []
    91   2112.5 MiB      0.0 MiB           9                           for j in range(num_integrations):
    92   2112.5 MiB      0.0 MiB          12                               u_t = gaussian_process.eval_u_t(t_vec[-1], \
    93   2112.5 MiB      0.0 MiB           6                                           params_test[alpha_indx], HyperParams.T_f)
    94                                                                     
    95   2112.5 MiB      0.0 MiB           6                               dyn_input = torch.cat((u_t.unsqueeze(0), stn), dim=0)
    96   2112.5 MiB      0.0 MiB           6                               stn_derivative = model_dyn(dyn_input)
    97   2112.5 MiB      0.0 MiB           6                               stn = stn + HyperParams.dt * stn_derivative
    98   2112.5 MiB      0.0 MiB           6                               t_vec.append(t_vec[-1] + HyperParams.dt)
    99                                                                 # Go through the decoder
   100   2112.5 MiB      0.0 MiB           3                           data = get_data(test_loader, (alpha_indx*(len(times)-1)+t_indx))
   101                                                                 #data = data.to(device)
   102   2112.5 MiB      0.0 MiB           3                           decoder_input = torch.cat((u_t.unsqueeze(0), stn), dim=0)
   103   2112.5 MiB      0.0 MiB           3                           out = model_decoder(decoder_input, data)
   104   2112.5 MiB      0.0 MiB           3                           loss_test_mse += physics_loss(data.x, out, HyperParams)
   105   2112.5 MiB      0.0 MiB           3                           total_examples += 1
   106                                                                 
   107   2112.5 MiB      0.0 MiB           1                   loss_test_mse = loss_test_mse/total_examples
   108   2112.5 MiB      0.0 MiB           1                   test_rmse = loss_test_mse
   109   2112.5 MiB      0.0 MiB           1                   test_history['test'].append(loss_test_mse.item())
   110                                         
   111                                         
   112                                                     # print("Epoch[{}/{}, train_mse loss:{}, test_mse loss:{}".format(epoch + 1, HyperParams.max_epochs, train_history['train'][-1], test_history['test'][-1]))
   113   2112.5 MiB      0.0 MiB           1               loop.set_postfix({"Loss(training)": train_history['train'][-1], "Loss(validation)": test_history['test'][-1]})
   114                                                 else:
   115                                                     test_rmse = train_rmse
   116                                                     # print("Epoch[{}/{}, train_mse loss:{}".format(epoch + 1, HyperParams.max_epochs, train_history['train'][-1]))
   117                                                     loop.set_postfix({"Loss(training)": train_history['train'][-1]})
   118                                         
   119                                                 # if test_rmse < min_test_loss:
   120                                                 #     min_test_loss = test_rmse
   121                                                 #     best_epoch = epoch
   122                                                 #     torch.save(model_decoder.state_dict(), HyperParams.net_dir+HyperParams.net_name+HyperParams.net_run+'_decoder.pt')
   123                                                 #     torch.save(model_dyn.state_dict(), HyperParams.net_dir+HyperParams.net_name+HyperParams.net_run+'_dyn.pt')
   124   2112.7 MiB      0.1 MiB           1           if HyperParams.tolerance >= train_rmse:
   125                                                     print('Early stopping!')
   126                                                     break
   127   2112.7 MiB      0.0 MiB           1           np.save(HyperParams.net_dir+'history'+HyperParams.net_run+'.npy', train_history)
   128   2112.7 MiB      0.0 MiB           1           np.save(HyperParams.net_dir+'history_test'+HyperParams.net_run+'.npy', test_history)
   129                                                
   130                                             #print("\nLoading best network for epoch: ", best_epoch)
   131   2112.8 MiB      0.1 MiB           1           torch.save(model_decoder.state_dict(), HyperParams.net_dir+HyperParams.net_name+HyperParams.net_run+'_decoder.pt')
   132   2112.8 MiB      0.0 MiB           1           torch.save(model_dyn.state_dict(), HyperParams.net_dir+HyperParams.net_name+HyperParams.net_run+'_dyn.pt')
   133   2112.8 MiB      0.0 MiB           1       model_decoder.load_state_dict(torch.load(HyperParams.net_dir+HyperParams.net_name+HyperParams.net_run+'_decoder.pt', map_location=torch.device('cpu')))
   134   2112.8 MiB      0.0 MiB           1       model_dyn.load_state_dict(torch.load(HyperParams.net_dir+HyperParams.net_name+HyperParams.net_run+'_dyn.pt', map_location=torch.device('cpu')))


